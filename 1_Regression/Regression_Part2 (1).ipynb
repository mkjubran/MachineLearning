{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Regression_Part2.ipynb","provenance":[{"file_id":"1X5qoSXEoG5mQmPezcXgMK6EW6MXq6YrM","timestamp":1577730423734}],"private_outputs":true,"collapsed_sections":["9f-VPoRayLM7","hekbanaGUK6M"],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"MzmEcNHzuBI3","colab_type":"text"},"source":["# Clone the Source GitHub Reporsitory \n","We need to clone some source files to be used throughtout this tutorial from a GitHub reprository"]},{"cell_type":"code","metadata":{"id":"TmP4GrRNudXH","colab_type":"code","colab":{}},"source":["!rm -rf ./MachineLearning\n","!git clone https://github.com/mkjubran/MachineLearning.git"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DIlzJbCpmo0R","colab_type":"text"},"source":["# One Hot Encoding\n","**Introduction**\n","\n","In this section, we will apply multiple leaner regression to a categorical data. \n","\n","We will be using the **One hot encoding** to convert nominal categorical variables into a form that could be provided to ML algorithms for linear regression."]},{"cell_type":"markdown","metadata":{"id":"JeQ0wuIc1AHR","colab_type":"text"},"source":["**Theory** \\\\\n","\n","One hot encoding is a process by which nominal categorical variables are converted into a form that could be provided to ML algorithms to do a better job in prediction.[1]\n","\n","Say suppose the dataset is as follows:\n","\n","City | Area| Price\n","--- | --- | ---\n","Jerusalem | 160 | 550000\n","Jerusalem | 200 | 600000\n","Jerusalem | 250 | 620000\n","Ramallah | 160 | 200000\n","Ramallah | 200 | 220000\n","Ramallah | 240 | 300000\n","Nablus | 160 | 150000\n","Nablus | 230 | 180000\n","Bethlehem | 160 | 160000\n","Bethlehem | 210 | 180000\n","\n","\n","We need to encode the names of the cities before passing this data into a machine learning model. This can be achieved through integer encoding as follows:\n","\n","City | Code | Area| Price\n","--- | --- | --- | ---\n","Jerusalem |0| 160 | 550000\n","Jerusalem |0| 200 | 600000\n","Jerusalem |0| 250 | 620000\n","Ramallah  |1| 160 | 200000\n","Ramallah  |1| 200 | 220000\n","Ramallah  |1| 240 | 300000\n","Nablus    |2| 160 | 150000\n","Nablus    |2| 230 | 180000\n","Bethlehem |3| 160 | 160000\n","Bethlehem |3| 210 | 180000\n","\n","\n","However, Ml might understand that Nablus is double Ramallah or Bethlehem is triple of Ramallah. But this categorical variable is not nominal (values don't exhibit any order as compared to ordinal variables) . so instead of this, we use **one hot coding** as follows:\n","\n","City | Jerusalem | Ramallah | Nablus| Bethlehem | Area| Price\n","--- | --- | --- | --- | --- | --- | ---\n","Jerusalem |1|0|0|0| 160 | 550000\n","Jerusalem |1|0|0|0| 200 | 600000\n","Jerusalem |1|0|0|0| 250 | 620000\n","Ramallah  |0|1|0|0| 160 | 200000\n","Ramallah  |0|1|0|0| 200 | 220000\n","Ramallah  |0|1|0|0| 240 | 300000\n","Nablus    |0|0|1|0| 160 | 150000\n","Nablus    |0|0|1|0| 230 | 180000\n","Bethlehem |0|0|0|1| 160 | 160000\n","Bethlehem |0|0|0|1| 210 | 180000\n","\n","As can be seen, four independent variables (dummy variables) are created; Jerusalem, Ramallah, Nablus, and Bethlehem. Each of these dummy variables encodes its city by \"1\" otherwise it is \"0\". \n","\n","Before passing this table to the ML, we need to remove one of the city columns because it is not needed and also cause what is called **Dummy variable trap**$^{[2]}$; say we remove the dummy variable Ramallah, so if none of the other dummy variables (Jerusalem, Nablus, and  Bethlehem) is \"1\" then the ML learn it is Ramallah. The Dummy variable trap occurs when one dummy variable can be predicted using the other dummy variables. Reducing the dimensionality of the dataset also reduces the complexity and time of training the model. To read further about one hot coding you may refer to [1].\n","\n","[1] https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f\n","[2] https://analyticstraining.com/understanding-dummy-variable-traps-regression/"]},{"cell_type":"markdown","metadata":{"id":"_RSwASngm9_9","colab_type":"text"},"source":["**Implementation**\n","\n","Read the input data from a csv file called \"homeprices_OHE.csv\" \\\\\n","To read the data in the file, we will be using the pandas library (https://pandas.pydata.org/)."]},{"cell_type":"code","metadata":{"id":"fQX2iq_fnJOm","colab_type":"code","colab":{}},"source":["import pandas as pd\n","df = pd.read_csv(\"./MachineLearning/1_Regression/homeprices_OHE.csv\")\n","print(df)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rJA9kY1o6_lr","colab_type":"text"},"source":["As can be seen, one of the fields (city) contains nominal categorical variable. Thus we need to encode this field into numeric values using one-hot coding. We wil use the pd.get_dummies(df.city) method as"]},{"cell_type":"code","metadata":{"id":"ryh3BJOV7eo1","colab_type":"code","colab":{}},"source":["dm = pd.get_dummies(df.city)\n","dm"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"39M2I3LU70dB","colab_type":"text"},"source":["After executing the above command we get a table with a code per city. Now we need to concatenate these rows to the original (df) dataframe."]},{"cell_type":"code","metadata":{"id":"dtqBhjVN8Hgl","colab_type":"code","colab":{}},"source":["df_merge = pd.concat([df,dm],axis='columns')\n","print(df_merge)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3HHv5Gko8_20","colab_type":"text"},"source":["Now we need to get the multiple regression model. Note we pass the area and the three city dummy variables to train the model. "]},{"cell_type":"code","metadata":{"id":"sBkR7WTW9T7j","colab_type":"code","colab":{}},"source":["from sklearn import linear_model\n","regm = linear_model.LinearRegression()\n","regm.fit(df_merge[['area','Bethlehem','Jerusalem','Nablus']],df_merge.price)\n","print(regm.coef_) ## print the coefficients\n","print(regm.intercept_) ## print the intercept"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Yg0bQjS5CMlB","colab_type":"text"},"source":["Alternatively, we could clean the data frame by dropping the not needed fields from the data frame and then define the inout variables to the modelas "]},{"cell_type":"code","metadata":{"id":"eCfpxkOSCuRq","colab_type":"code","colab":{}},"source":["x= df_merge.drop(['Ramallah','city','price'],axis=1)\n","print(x)\n","y = df_merge.price\n","print(y)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-eXLcFhJE1DU","colab_type":"text"},"source":["To train the model using x and y:"]},{"cell_type":"code","metadata":{"id":"oaa_QikPEzwz","colab_type":"code","colab":{}},"source":["regm.fit(x,y)\n","print(regm.coef_) ## print the coefficients\n","print(regm.intercept_) ## print the intercept"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Zd0a13CuAMmf","colab_type":"text"},"source":["The model is now ready. To estimate the price of a new house in Ramallah with an area of 190 $m^2$, we apply it to the model as follows:"]},{"cell_type":"code","metadata":{"id":"nuvM1TH8Anv1","colab_type":"code","colab":{}},"source":["regm.predict([[190,0,0,0]])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gV6Pc0FJL7AG","colab_type":"text"},"source":["So the price of such a house is about $232112. Let us next compare the prices of houses of the same size (area) in different cities. Use the city code based on the one hot coding shwon in output cell [37]. "]},{"cell_type":"code","metadata":{"id":"Fv4n1EhxMVK7","colab_type":"code","colab":{}},"source":["x_new=[[190,1,0,0],[190,0,1,0],[190,0,0,1],[190,0,0,0]]\n","regm.predict(x_new)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Sio0MGhXM5KO","colab_type":"text"},"source":["So the prices of houses with an area of 190 $m^2$ is as follows:\n","\n","City | Price\n","--- | ---\n","Bethlehem | 173943.76899694 \n","Jerusalem | 579483.28267475\n","Nablus | 161056.23100302\n","Ramallah | 232112.46200606"]},{"cell_type":"markdown","metadata":{"id":"8rFQOBBCtqlE","colab_type":"text"},"source":["**Exercise**\n","\n","Use multiple linear regression to estimate the prices of the following cares:\n","1- ![image.png](data:image/png;base64,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)BMW car model 2011\n","2- Marcedes model 2010\n","3- Audi model 2015\n","5- Hundai model 2018\n","Use the data set in the 'CarPrices.csv' file to train the model."]},{"cell_type":"markdown","metadata":{"id":"9f-VPoRayLM7","colab_type":"text"},"source":["# Multiple Regression\n","**Introduction**\n","\n","In this section, we will extend the model derives in the Linear Regression section to include more than one independent variable. This method is called Multiple Regression. We will use multiple properties of the house (area, number of rooms, age) to predict its price."]},{"cell_type":"markdown","metadata":{"id":"FWhZGsyYNEPV","colab_type":"text"},"source":["**Theory**\n","\n","Multiple regression is an extension of simple linear regression. It is used when we want to predict the value of a variable based on the value of two or more other variables. The variable we want to predict is called the dependent variable (or sometimes, the outcome, target or criterion variable). The variables we are using to predict the value of the dependent variable are called the independent variables (or sometimes, the predictor, explanatory or regressor variables). For example, you could use multiple regression to understand whether exam performance can be predicted based on revision time, test anxiety, lecture attendance and gender. $^{[2]}$\n","\n","[2] https://statistics.laerd.com/spss-tutorials/multiple-regression-using-spss-statistics.php"]},{"cell_type":"markdown","metadata":{"id":"XsWik1emdqRJ","colab_type":"text"},"source":["**Implementation**\n","\n","Read the data in the file \"HousesPrices.csv\" using pandas libarary."]},{"cell_type":"code","metadata":{"id":"jL4EcjFsdp0m","colab_type":"code","colab":{}},"source":["import pandas as pd\n","df = pd.read_csv(\"./MachineLearning/1_Regression/HousesPrices.csv\")\n","print(df)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HAsnCGlEhh0_","colab_type":"text"},"source":["In the above table, there is more than one feature that corresponds to the price of each house. In the multivariate, the number of input independent variables (features) is at least two or more. In our case, the features are area, number of bedrooms, and age of the house. And the dependent variable is the price of the house. \n","\n","\n","![alt text](https://drive.google.com/uc?id=1a4tq7w_mewJ3gUykvvT5PiOg4rXKsBMJ)\n","\n","\n","We notice that there is a NaN number of bedrooms next to house index 2, this is typically due to empty value in the csv file. Thus, we need to process the dataframe to clean the data. In our case, we will replace the NaN value with the median of the other bedroom values in the table."]},{"cell_type":"code","metadata":{"id":"p415q6o7ma7y","colab_type":"code","colab":{}},"source":["import math\n","median_bedrooms = df.bedrooms.median() # media of number of bedrooms in dataframe\n","print(median_bedrooms)\n","median_bedrooms = math.floor(df.bedrooms.median())# use the math library to compute the floor of the media of number of bedrooms in dataframe\n","print(median_bedrooms)\n","df.bedrooms = df.bedrooms.fillna(median_bedrooms) #replace the NAN with the median value\n","print(df)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-1Fmyz__mPvw","colab_type":"text"},"source":["Now the data is clean. Next, we will use the LinearRegression method in the sklearn library (https://scikit-learn.org/stable/) to derive the best fitting line (determine the best coefficients and interception values) based on the given data. \n"]},{"cell_type":"code","metadata":{"id":"KEIeK_ZChge4","colab_type":"code","colab":{}},"source":["from sklearn import linear_model\n","regm = linear_model.LinearRegression()\n","regm.fit(df[['area','bedrooms','age']],df.price)\n","print(regm.coef_) ## print the coefficients\n","print(regm.intercept_) ## print the intercept"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"siDScZ1Q4nFs","colab_type":"text"},"source":["After building the model, we will use it to estimate the prices of a list of houses based on their features (area, number of bedrooms, age). Let us assume features of few houses are stored in a csv file called \"HousesFeatures.csv\". We will read the data from the file into a dataframe, and apply the values of the features in the dataframe to the model to determine the estimated prices, then we will append the etimted prices to the dataframe and store the new dataframe to new csv file called \"PredictedHouusesFeaturesPrices.csv\""]},{"cell_type":"code","metadata":{"id":"DA5CQ5Qy6TVp","colab_type":"code","colab":{}},"source":["dfm = pd.read_csv(\"./MachineLearning/1_Regression/HousesFeatures.csv\")\n","p=regm.predict(dfm)\n","dfm['price']=p\n","print(dfm)\n","dfm.to_csv('./MachineLearning/1_Regression/PredictHousesFeaturesPrices.csv',index=False)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H_uYKRTART1F","colab_type":"text"},"source":["Optional: we could also check the residul error between the actual prices (in 'HousesPrices.csv') and the predicted values. "]},{"cell_type":"code","metadata":{"id":"5Ry8163yR5PS","colab_type":"code","colab":{}},"source":["ppr=regm.predict(df[['area','bedrooms','age']])\n","df['predicted_prices']=ppr\n","df['Residual']=df['price']-df['predicted_prices']\n","print(df)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rAX3BodQ7OcN","colab_type":"text"},"source":["**Exercise (1)**\n","Use multiple regression to estimate the prices of the houses based on houses' features. To complete this exercise, two files are included in the repository: \\\\\n","1- HousesPrices_Exercise.csv: a list of houses' prices and their features \\\\\n","2- HousesFeatures_Exercise.csv: a list of ages of houses\n","\n","hint: use the word2number (https://pypi.org/project/word2number/) library to convert number words (eg. twenty one) to numeric digits (21).\n","\n","**Exercise (2)**\n","Use multiple regression to estimate the salary of a person based on experience, test results, and interview score. You are given some data in the \"hiring.csv\" file included in the repository. You need to propose a salary for the following two persons: \n","\n","**Person 1**: nine years of experience, 9 in the test score, and 6 in the interview, \\\\\n","**Person 2**: twelve years of experience, 10 in the test score, and 9 in the interview "]},{"cell_type":"markdown","metadata":{"id":"hekbanaGUK6M","colab_type":"text"},"source":["# Cost Function and Gradient Descent\n","In this section, we will learn how to use gradient descent to determine the optimal coefficients and intercept of linear regression."]},{"cell_type":"markdown","metadata":{"id":"YjbP4FeiUqRZ","colab_type":"text"},"source":["**Theory**\n","\n","In Machine Learning (ML), cost function is a measure of how wrong the model is in terms of its ability to estimate the relationship between the independent variable (x) and the dependent variable (y). This is typically expressed as a difference or distance between the predicted value and the actual value.$^{[1]}$ One common cost function that is often used and will be used in this sesisons is mean squared error (MSE), which measures the difference between the ground truth ($y_i$) and the estimated value ($\\hat{y}_i$). \\\\\n","\n","\\begin{equation}\n","\\begin{aligned}\n","MSE=\\frac{1}{n} \\sum^n_{i=1}{(y_i -\\hat{y}_i)^2}   \n","\\end{aligned}\n","\\end{equation}\n","\n","The cost function (you may also see this referred to as loss or error.) can be estimated by iteratively running the model to compare estimated predictions against “ground truth” — the known values of y. The objective of a ML model, therefore, is to find parameters, weights or a structure that minimises the cost function.$^{[1]}$ Gradient descent is an efficient optimization algorithm that attempts to find a local or global minima of a function. **Gradient descent** enables a model to learn the gradient or direction that the model should take in order to reduce errors (differences between actual y and predicted y).$^{[1]}$\n","\n","![alt text](https://drive.google.com/uc?id=1fRW5deq8-LDrJcA537TvCOpDvChSk1pa)\n","\n","In the linear regression case, we need to determine the values of **m** and **b** that minimize the cost function (**MSE**) as shown below. \n","\n","![alt text](https://drive.google.com/uc?id=1-djT6TUolxA_C5eDiX0hIAZcD7M68vCn)\n","\n","We will use the gradient descent to determine the direction and step size to progress from an initial point toward the global minimum of the **MSE**. \n","![alt text](https://drive.google.com/uc?id=16MCwuGihzmVaRYziuy5jxvWnE5m1sr_3)\n","\n","Given a set of data point, below is a visualization of how the gradient descent works $^{2}$\n","\n","\n","![alt text](https://drive.google.com/uc?id=11MmCe-tEwK_SQ-qwwbc4ZY2QpE8GhIL4)\n","\n","\n","[1] https://towardsdatascience.com/machine-learning-fundamentals-via-linear-regression-41a5d11f5220\n","\n","[2] https://github.com/mattnedrich/GradientDescentExample/blob/master/gradient_descent_example.gif\n"]},{"cell_type":"markdown","metadata":{"id":"Fb8rhJC7W1Ap","colab_type":"text"},"source":["**Readings and Resources** \\\\\n","1- https://towardsdatascience.com/machine-learning-fundamentals-via-linear-regression-41a5d11f5220\n","\n","2- https://medium.com/@lachlanmiller_52885/machine-learning-week-1-cost-function-gradient-descent-and-univariate-linear-regression-8f5fe69815fd"]},{"cell_type":"markdown","metadata":{"id":"o2fMSaUmUbDP","colab_type":"text"},"source":["**Implementation**\n","\n","In order to determine the best fit line, we need to determine the values of **m** and **b** of the straight line $\\hat{y}_i=mx_i+b$ that minimze the MSE. \n","\n","\\begin{equation}\n","\\begin{aligned}\n","MSE=J=\\frac{1}{n} \\sum^n_{i=1}{(y_i -\\hat{y}_i)^2}   \n","\\end{aligned}\n","\\end{equation}\n","\n","So we substitute $\\hat{y}_i=mx_i+b$ into the cost function as\n","\n","\n","\\begin{equation}\n","\\begin{aligned}\n","J=\\frac{1}{n} \\sum^n_{i=1}{(y_i -mx_i+b)^2}   \n","\\end{aligned}\n","\\end{equation}\n","\n","Then, we determine the gradient by taking the partial derivative of the cost function with respect to **m** and **b** as\n","\n","\\begin{equation}\n","\\begin{aligned}\n","\\frac{\\partial J}{\\partial m}=\\frac{2}{n} \\sum^n_{i=1}{(y_i -mx_i+b) \\times (-x_i)} \n","\\end{aligned}\n","\\end{equation}\n","\n","\\begin{equation}\n","\\begin{aligned}\n","\\frac{\\partial J}{\\partial b}=\\frac{2}{n} \\sum^n_{i=1}{(y_i -mx_i+b) \\times (-1)} \n","\\end{aligned}\n","\\end{equation}\n","\n","So now to implement the gradient descent, we start with some values of **m** ($m_0$) and **b** ($b_0$) and iteratively modify them according the gradient and learning rate ($\\lambda$) as follows:\n","\n","\\begin{equation}\n","\\begin{aligned}\n","m_i = m_{i-1} - \\lambda \\times \\frac{\\partial J}{\\partial m} \n","\\end{aligned}\n","\\end{equation}\n","\n","\\begin{equation}\n","\\begin{aligned}\n","b_i = b_{i-1} - \\lambda \\times \\frac{\\partial J}{\\partial b} \n","\\end{aligned}\n","\\end{equation}\n"]},{"cell_type":"code","metadata":{"id":"2MK89QcjYqdO","colab_type":"code","colab":{}},"source":["import numpy as np\n","def gradient_descent_basic(x,y,m_curr,b_curr,learning_rate,iterations):\n","    n = len(x)\n","    for i in range(iterations):\n","        y_pred = m_curr * x + b_curr\n","        \n","        md = - ( 2 / n ) * sum( x * ( y - y_pred ))\n","        bd = - ( 2 / n ) * sum(( y - y_pred ))\n","\n","        m_curr = m_curr - learning_rate * md \n","        b_curr = b_curr - learning_rate * bd \n","\n","        J = ( 1 / n ) * sum(( y - y_pred )**2)\n","\n","        print('J = {}, m = {}, b = {}, Iteration = {}'.format(J ,m_curr, b_curr, i ))\n","    return m_curr,b_curr,i,J\n","\n","## try the gradient_descent using sample data\n","x = np.array([0,1,2,3]);\n","y = np.array([1,3,5,7]); ## y=2x+1\n","\n","m_curr = 0; b_curr = 0;\n","gradient_descent_basic(x,y,m_curr,b_curr,0.2,20) ## learning rate = 0.2 and iteration = 20"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XcFNmc4zeRtR","colab_type":"text"},"source":["Let us increase learning rate to 0.5 and see how the gradient descent converges."]},{"cell_type":"code","metadata":{"id":"Pms6CmU-ei8G","colab_type":"code","colab":{}},"source":["## try the gradient_descent with learning rate = 0.5 and iteration = 20\n","m_curr = 0; b_curr = 0;\n","gradient_descent_basic(x,y,m_curr,b_curr,0.5,20)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3gOaimdMetgm","colab_type":"text"},"source":["As can be seen, the cost function increases instead of descreasing.\n","\n","![alt text](https://drive.google.com/uc?id=1Urf6nAJ0-G5miH1EdCk4gCo5ctBtqVTh)\n","\n","So usually, we start with low iteration value and some value of learning rate and see if the cost function is reducing.  Then we increase the learning rate slowly to the value just before the cost function starts increasing. This value is the best learning rate (converge with the least number of iterations).\n","\n","Regarding the required number of iterations, you may stop the gradient descent search once the difference in the cost function between successive iterations reduces to less than some value (such as 1e-5 or 1e-6). Next we will modify the code to stop when the error (MSE) is less than 1e-6."]},{"cell_type":"code","metadata":{"id":"Jo6-diDXwB9W","colab_type":"code","colab":{}},"source":["import numpy as np\n","import copy\n","def gradient_descent(x,y,m_curr,b_curr,learning_rate,epochs):\n","    n = len(x)\n","    i = 0 \n","    j_curr = 100000\n","    while True:\n","        i= i + 1\n","        j_before = j_curr\n","        y_pred = m_curr * x + b_curr\n","        \n","        md = - ( 2 / n ) * sum( x * ( y - y_pred ))\n","        bd = - ( 2 / n ) * sum( y - y_pred )\n","\n","        m_curr = m_curr - learning_rate * md \n","        b_curr = b_curr - learning_rate * bd \n","\n","        j_curr = ( 1 / n ) * sum(( y - y_pred )**2)\n","\n","        if ((abs(j_curr - j_before) < 1e-5) or (i >= epochs)):\n","          return m_curr,b_curr,i,j_curr\n","\n","## try the gradient_descent using sample data\n","x = np.array([0,1,2,3]);\n","y = np.array([1,3,5,7]); ## y=2x+1\n","\n","m_curr = 0; b_curr = 0;\n","gradient_descent(x,y,m_curr,b_curr,0.2,100) ## learning rate = 0.2 and iteration = 20"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FQJ2qOQhpA97","colab_type":"text"},"source":["Next, we will solve the original problem (linear regression section) using our gradient descent implementation: \\\\\n","1- Read the data in the file \"HousesAreasPrices.csv\" using pandas libarary, \\\\\n","2- convert the fields in the data frame to np.arrays, \\\\\n","3- then we apply the gradient_descent(x,y) on the np.arrays. \\\\\n","Recall, the solution we got in the linear regression section is **m** = 135.78767123 and **b** = 180616.43835616432"]},{"cell_type":"markdown","metadata":{"id":"_tdO_CtTnHhm","colab_type":"text"},"source":["Let us begin by determining the learning rate. We will use gradient_descent_basic() to print error while trying different learning rate values."]},{"cell_type":"code","metadata":{"id":"H3NI9Y7RmhzV","colab_type":"code","colab":{}},"source":["import pandas as pd\n","df = pd.read_csv(\"./MachineLearning/1_Regression/HousesAreasPrices.csv\")\n","print(df)\n","x=np.array(df.area)\n","y=np.array(df.price)\n","## change the learning rate and iterations\n","m_gd, b_gd, iters, j_curr= gradient_descent_basic(x,y,0,0,0.00000001,20)\n","print('J= {}, m = {}, b = {}, iterations = {}'.format(j_curr, m_gd, b_gd,iters))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UqN0-ca6n0cq","colab_type":"text"},"source":["As can be observed, we need to use very low learning rate to make sure error is decreasing. However, such a low learning rate needs a lot of iterations to converge. To deal with this we apply data scaling. So we scale the independent random variable according to its mean and standard deviations. "]},{"cell_type":"code","metadata":{"id":"CtQvQovRoZo1","colab_type":"code","colab":{}},"source":["import pandas as pd\n","df = pd.read_csv(\"./MachineLearning/1_Regression/HousesAreasPrices.csv\")\n","print(df)\n","x=np.array(df.area)\n","y=np.array(df.price)\n","\n","## scaling the independent random variable\n","x_new = (x - np.mean(x)) / np.std(x)\n","\n","## change the learning rate and iterations\n","m_gd, b_gd, iters, j_curr= gradient_descent_basic(x_new,y,0,0,0.1,20)\n","print('J = {}, m = {}, b = {}, iterations = {}'.format(j_curr, m_gd, b_gd,iters))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Yp7Ih9WmqrF7","colab_type":"text"},"source":["Now, to find the best coefficients, we increase the number of iterations."]},{"cell_type":"code","metadata":{"id":"3YTt_fwzqzIQ","colab_type":"code","colab":{}},"source":["## change the learning rate and iterations\n","m_gd, b_gd, iters, j_curr= gradient_descent(x_new,y,0,0,0.01,20000)\n","print('J = {}, m = {}, b = {}, iterations = {}'.format(j_curr, m_gd, b_gd,iters))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RirEHpNwuc3H","colab_type":"text"},"source":["Notice that the number of iterations required such that the difference between MSE of successive iterations is less than 1e-5 is 868 only."]},{"cell_type":"markdown","metadata":{"id":"T-3pKQAYtzNd","colab_type":"text"},"source":["As an inclass exercise, we will compare the error function between the rg.fit method (linear regression section) and the gradient descent."]},{"cell_type":"code","metadata":{"id":"Y-bQ-1xg8dd0","colab_type":"code","colab":{}},"source":["m_reg = 135.78767123; ## from linear regression section\n","b_reg = 180616.43835616432; ## from linear regression section\n","\n","m_gd ## from gradient descient results in code cell above\n","b_gd ## from gradient descient results in code cell above\n","\n","y_actual = np.array(df.price)\n","y_pred_reg = m_reg * x + b_reg;\n","y_pred_gd = m_gd * x_new + b_gd;\n","\n","n=len(y_actual)\n","\n","J_reg = (1/n)*sum(abs(y_actual - y_pred_reg));\n","J_gd = (1/n)*sum(abs(y_actual - y_pred_gd));\n","\n","dif = J_reg - J_gd;\n","\n","print('J_reg = {}, J_gd = {}, Difference = {}'.format(J_reg, J_gd, dif))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a3JbTFOTDVZn","colab_type":"text"},"source":["As can be seen the coefficients are not the same. However the difference between MSE of both methods is very small. Let us try to plot the reg.fit line and gradient descent line on the same plot."]},{"cell_type":"code","metadata":{"id":"r3E1OOkaudEy","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","plt.scatter(df.area,df.price,color='r', marker='+')\n","plt.xlabel('Area ($m^2$)',fontsize=20)\n","plt.ylabel('Prices ($)',fontsize=20)\n","plt.plot(df.area,y_pred_reg,color='b',label='reg.fit') ## best fit line using reg.predict\n","plt.plot(df.area,y_pred_gd,color='g',linestyle='--',linewidth=3,label='Gradient Descent') ## best fit line using gradient descent\n","plt.legend()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u8rvtRECsIlv","colab_type":"text"},"source":["As can be seen, the reg.fit and the gradient descent lines are exactly the same.  "]},{"cell_type":"markdown","metadata":{"id":"Lx8B1I8uwnZa","colab_type":"text"},"source":["**Exercise**\n","\n","Modify the gradient descent to be used for multiple linear regression with three independent variables. Then use it to estimate the houses' prices given the area, number of bedrooms, and age discussed in the multiple linear regression section."]},{"cell_type":"markdown","metadata":{"id":"O5Gu6ZTw3PJ_","colab_type":"text"},"source":["# Saving and Loading Training Models\n","\n","In this section we will learn how to save and load training models. We will do that using two methods; Pickle and Joblib.\n"]},{"cell_type":"markdown","metadata":{"id":"GzftRGLU3ty1","colab_type":"text"},"source":["First, we will create and train the linear regression model usied in the linear regresison section. "]},{"cell_type":"code","metadata":{"id":"0zUERr8q4CJH","colab_type":"code","colab":{}},"source":["import pandas as pd\n","from sklearn import linear_model\n","\n","df = pd.read_csv(\"./MachineLearning/1_Regression/HousesAreasPrices.csv\")\n","print(df)\n","\n","reg = linear_model.LinearRegression()\n","reg.fit(df[['area']],df.price)\n","print(reg.coef_) ## print the coefficient\n","print(reg.intercept_) ## print the intercept\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0oYlj-Mo4P1w","colab_type":"text"},"source":["Now, we will save the **reg** linear model using **pickle** library (https://docs.python.org/3/library/pickle.html)."]},{"cell_type":"code","metadata":{"id":"ul_be_YY40gc","colab_type":"code","colab":{}},"source":["import pickle\n","with open('./HomePricesLinearModel.pickle','wb') as f:\n","  pickle.dump(reg,f)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i-u4ySwU5uQ3","colab_type":"text"},"source":["The **HomePricesLinearModel** is saved at your current directory (.\\content\\) and includes the linear regression model. It doesn't include the dataframes or any other libraries."]},{"cell_type":"markdown","metadata":{"id":"fEQNkqvn6Q1U","colab_type":"text"},"source":["To load the model, we will use another method from the pickle library as:"]},{"cell_type":"code","metadata":{"id":"0gTdJ8XA6gVB","colab_type":"code","colab":{}},"source":["with open('./HomePricesLinearModel.pickle','rb') as f:\n","  reg_pickle = pickle.load(f)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JzgBHn8Q7ojg","colab_type":"text"},"source":["We could now use the new model to predict the prices of houses based on their areas as done before."]},{"cell_type":"code","metadata":{"id":"PtU8cIiD7zh_","colab_type":"code","colab":{}},"source":["df2 = pd.read_csv(\"./MachineLearning/1_Regression/HousesAreas.csv\")\n","print(df2)\n","p=reg_pickle.predict(df2)\n","df2['price']=p\n","df2.head()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fAOSVD-6-D5T","colab_type":"text"},"source":["Another approach to save the reg linear model is by using joblib from sklearn library (https://scikit-learn.org/stable/modules/model_persistence.html) as:"]},{"cell_type":"code","metadata":{"id":"r_G2PA2h-9Vm","colab_type":"code","colab":{}},"source":["import joblib as jb\n","jb.dump(reg, './HomePricesLinearModel.joblib') "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zgnn_Wnp_c1C","colab_type":"text"},"source":["In this case there is no need to open file before dumping the data. similarly for loading data."]},{"cell_type":"code","metadata":{"id":"TUg8dgHB_nyG","colab_type":"code","colab":{}},"source":["reg_joblib = jb.load('./HomePricesLinearModel.joblib')\n","\n","df2 = pd.read_csv(\"./MachineLearning/1_Regression/HousesAreas.csv\")\n","print(df2)\n","p=reg_joblib.predict(df2)\n","df2['price']=p\n","df2.head()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iehJMKtBANxW","colab_type":"text"},"source":["If you need to learn more about pickle and joblib refer to https://scikit-learn.org/stable/modules/model_persistence.html ."]}]}